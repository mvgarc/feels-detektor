{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be771e88-4f06-4dea-8fff-15478813d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57350b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175e1030-13b3-4589-9bc2-6201d2aa6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(directory):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for lbl in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, lbl)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            continue\n",
    "        for imagename in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, imagename))\n",
    "            labels.append(lbl)\n",
    "        print(lbl, \"complete\")\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d45ee8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry complete\n",
      "disgust complete\n",
      "fear complete\n",
      "happy complete\n",
      "neutral complete\n",
      "sad complete\n",
      "surprise complete\n"
     ]
    }
   ],
   "source": [
    "images, labs = create_dataframe(TRAIN_DIR)\n",
    "train = pd.DataFrame({'image': images, 'label': labs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd48e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05acfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry complete\n",
      "disgust complete\n",
      "fear complete\n",
      "happy complete\n",
      "neutral complete\n",
      "sad complete\n",
      "surprise complete\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = create_dataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd97f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    image     label\n",
      "0       images/validation\\angry\\10052.jpg     angry\n",
      "1       images/validation\\angry\\10065.jpg     angry\n",
      "2       images/validation\\angry\\10079.jpg     angry\n",
      "3       images/validation\\angry\\10095.jpg     angry\n",
      "4       images/validation\\angry\\10121.jpg     angry\n",
      "...                                   ...       ...\n",
      "7061  images/validation\\surprise\\9806.jpg  surprise\n",
      "7062  images/validation\\surprise\\9830.jpg  surprise\n",
      "7063  images/validation\\surprise\\9853.jpg  surprise\n",
      "7064  images/validation\\surprise\\9878.jpg  surprise\n",
      "7065   images/validation\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/validation\\angry\\10052.jpg\n",
      "1         images/validation\\angry\\10065.jpg\n",
      "2         images/validation\\angry\\10079.jpg\n",
      "3         images/validation\\angry\\10095.jpg\n",
      "4         images/validation\\angry\\10121.jpg\n",
      "                       ...                 \n",
      "7061    images/validation\\surprise\\9806.jpg\n",
      "7062    images/validation\\surprise\\9830.jpg\n",
      "7063    images/validation\\surprise\\9853.jpg\n",
      "7064    images/validation\\surprise\\9878.jpg\n",
      "7065     images/validation\\surprise\\993.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (test)\n",
    "print (test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32110e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "181d5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        # load_img: use color_mode='grayscale' and resize to 48x48\n",
    "        img = load_img(image, color_mode='grayscale', target_size=(48,48))\n",
    "        arr = np.array(img, dtype=np.float32)\n",
    "        # ensure channel dimension exists (H, W, 1)\n",
    "        if arr.ndim == 2:\n",
    "            arr = arr[..., np.newaxis]\n",
    "        features.append(arr)\n",
    "    features = np.stack(features, axis=0)\n",
    "    # normalize to [0,1]\n",
    "    features /= 255.0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "570ed5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[AException ignored in: <function tqdm.__del__ at 0x000001E8888DC700>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 28821/28821 [00:05<00:00, 4810.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "802e4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7066/7066 [00:00<00:00, 7144.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate validation features from the validation/test dataframe\n",
    "validation_features = extract_features(test['image'])\n",
    "# extract_features already normalizes to [0,1], so do not divide again\n",
    "x_train = train_features\n",
    "x_validation = validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8a926d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001E8888DC700>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001E8888DC700>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\valen\\Documents\\proyecto=feels\\feels-detektor\\.venv\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e5f436ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4a5cf766",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_validation = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "826f3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_validation = to_categorical(y_validation, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "06b16368",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  # convolutional layers\n",
    "from keras.layers import Input\n",
    "model.add(Input(shape=(48,48,1)))\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7ebe3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2d080270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 5s/step - accuracy: 0.2430 - loss: 1.8232 - val_accuracy: 0.2570 - val_loss: 1.8039\n",
      "Epoch 2/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 5s/step - accuracy: 0.2430 - loss: 1.8232 - val_accuracy: 0.2570 - val_loss: 1.8039\n",
      "Epoch 2/100\n",
      "\u001b[1m  4/226\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:11\u001b[0m 4s/step - accuracy: 0.2194 - loss: 1.7800"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m     ):\n\u001b[1;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\OneDrive\\Desktop\\Detector de Emociones\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x = x_train, y = y_train, epochs=100, batch_size=128, validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0c4c5074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "978c5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0689f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('emotiondetector.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('emotiondetector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a1d27e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "54cebfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(images):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        img = load_img(image, color_mode='grayscale', target_size=(48,48))\n",
    "        arr = np.array(img, dtype=np.float32)\n",
    "        if arr.ndim == 2:\n",
    "            arr = arr[..., np.newaxis]\n",
    "        features.append(arr)\n",
    "    features = np.stack(features, axis=0)\n",
    "    features /= 255.0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b969f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "model predicted it as: Happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "model predicted it as: Happy\n"
     ]
    }
   ],
   "source": [
    "# Predict a single image: pass the path as a list to ef()\n",
    "image_path = 'images/train/sad/42.jpg'\n",
    "print('original image is sad')\n",
    "# ef expects an iterable of paths, so wrap in a list\n",
    "X = ef([image_path])  # returns shape (1,48,48,1)\n",
    "pred = model.predict(X)\n",
    "# get predicted class index for the first (and only) sample\n",
    "pred_idx = int(np.argmax(pred, axis=1)[0])\n",
    "pred_label = label[pred_idx]\n",
    "print(\"model predicted it as:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "33eb5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "73f0bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "model predicted it as: Happy\n",
      "model predicted it as: Happy\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict a single image: pass the path as a list to ef()\n",
    "image_path = 'images/train/sad/42.jpg'\n",
    "print('original image is sad')\n",
    "# ef expects an iterable of paths, so wrap in a list\n",
    "X = ef([image_path])  # returns shape (1,48,48,1)\n",
    "pred = model.predict(X)\n",
    "# get predicted class index for the first (and only) sample\n",
    "pred_idx = int(np.argmax(pred, axis=1)[0])\n",
    "pred_label = label[pred_idx]\n",
    "print(\"model predicted it as:\", pred_label)\n",
    "# display the image (squeeze channel dimension)\n",
    "img_arr = X[0].squeeze()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_arr, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c60b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
